{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines, and matplotlib for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "root = os.getcwd()\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "caffe_root = '/home/valeriyasin/Programms/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "os.chdir(caffe_root)\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaffeNet found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isfile(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'):\n",
    "    print 'CaffeNet found.'\n",
    "else:\n",
    "    print 'Downloading pre-trained CaffeNet model...'\n",
    "    !../scripts/download_model_binary.py ../models/bvlc_reference_caffenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_cpu()\n",
    "\n",
    "model_def = caffe_root + 'models/finetune_flickr_style/deploy.prototxt'\n",
    "model_weights = caffe_root +  'models/finetune_flickr_style/finetune_flickr_style.caffemodel'\n",
    "\n",
    "net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean-subtracted values: [('B', 104.0069879317889), ('G', 116.66876761696767), ('R', 122.6789143406786)]\n"
     ]
    }
   ],
   "source": [
    "# load the mean ImageNet image (as distributed with Caffe) for subtraction\n",
    "mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "print 'mean-subtracted values:', zip('BGR', mu)\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_file = caffe_root + 'examples/finetune_flickr_style/style_names.txt'\n",
    "if not os.path.exists(labels_file):\n",
    "    !../data/ilsvrc12/get_ilsvrc_aux.sh\n",
    "    \n",
    "labels = np.loadtxt(labels_file, str, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w') \n",
    "    results = []\n",
    "    for img in tqdm(X):\n",
    "        transformed_image = transformer.preprocess('data', img)\n",
    "        net.blobs['data'].data[...] = transformed_image\n",
    "        output = net.forward()\n",
    "        output_prob = output['prob'][0]\n",
    "        top_inds = output_prob.argsort()[::-1][:5]  # reverse sort and take five largest items\n",
    "        result = list(map(lambda x: {labels[x] : output_prob[x]}, top_inds))  \n",
    "        results.append(result)\n",
    "    sys.stdout = old_stdout\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_user(username):\n",
    "    photos = []\n",
    "    print(root + '/' +  username + '/*.jpg')\n",
    "    for img in glob.glob(root + '/' +  username + '/*.jpg'):\n",
    "        photo = imread(img, mode='RGB')\n",
    "        photo = imresize(photo, (photo.shape[0], photo.shape[1]))\n",
    "        photos.append(photo)\n",
    "    return predict(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:21<00:00,  1.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'Minimal': 0.17002888},\n",
       "  {'Geometric Composition': 0.14377806},\n",
       "  {'Melancholy': 0.11263728},\n",
       "  {'Detailed': 0.081270397},\n",
       "  {'Horror': 0.077205613}],\n",
       " [{'Bright': 0.26749131},\n",
       "  {'Geometric Composition': 0.15060575},\n",
       "  {'Texture': 0.091786653},\n",
       "  {'Detailed': 0.084351622},\n",
       "  {'Minimal': 0.071792185}],\n",
       " [{'Bright': 0.13020928},\n",
       "  {'Melancholy': 0.097577371},\n",
       "  {'Detailed': 0.070773289},\n",
       "  {'Minimal': 0.07056801},\n",
       "  {'Pastel': 0.060479585}],\n",
       " [{'Bright': 0.15789044},\n",
       "  {'Detailed': 0.128205},\n",
       "  {'Geometric Composition': 0.081471398},\n",
       "  {'Sunny': 0.069116235},\n",
       "  {'Texture': 0.068013184}],\n",
       " [{'Bright': 0.10442992},\n",
       "  {'Geometric Composition': 0.10275256},\n",
       "  {'Minimal': 0.09252172},\n",
       "  {'Detailed': 0.089410469},\n",
       "  {'Melancholy': 0.073703766}],\n",
       " [{'Bright': 0.1793323},\n",
       "  {'Minimal': 0.13623337},\n",
       "  {'Geometric Composition': 0.13514253},\n",
       "  {'Texture': 0.12205301},\n",
       "  {'Detailed': 0.063011646}],\n",
       " [{'Bright': 0.19942512},\n",
       "  {'Geometric Composition': 0.13437538},\n",
       "  {'Minimal': 0.099779718},\n",
       "  {'Sunny': 0.095239535},\n",
       "  {'Detailed': 0.076814085}],\n",
       " [{'Bright': 0.27703217},\n",
       "  {'Horror': 0.19017497},\n",
       "  {'Detailed': 0.1034738},\n",
       "  {'Melancholy': 0.095939666},\n",
       "  {'Texture': 0.04933466}],\n",
       " [{'Bright': 0.20034947},\n",
       "  {'Geometric Composition': 0.1190251},\n",
       "  {'Detailed': 0.11846031},\n",
       "  {'Texture': 0.05795259},\n",
       "  {'Melancholy': 0.05485335}],\n",
       " [{'Minimal': 0.14228889},\n",
       "  {'Horror': 0.13095985},\n",
       "  {'Noir': 0.11080243},\n",
       "  {'Bright': 0.089213334},\n",
       "  {'Geometric Composition': 0.080497958}],\n",
       " [{'Geometric Composition': 0.17531827},\n",
       "  {'Minimal': 0.13728045},\n",
       "  {'Bright': 0.12550822},\n",
       "  {'Detailed': 0.1132376},\n",
       "  {'Texture': 0.10687559}],\n",
       " [{'Geometric Composition': 0.15406699},\n",
       "  {'Minimal': 0.14395544},\n",
       "  {'Texture': 0.14142455},\n",
       "  {'Melancholy': 0.073230952},\n",
       "  {'Bright': 0.066912383}],\n",
       " [{'Horror': 0.19597042},\n",
       "  {'Melancholy': 0.11861064},\n",
       "  {'Minimal': 0.09183234},\n",
       "  {'Noir': 0.084812395},\n",
       "  {'Geometric Composition': 0.07741718}],\n",
       " [{'Bright': 0.2603319},\n",
       "  {'Horror': 0.16320737},\n",
       "  {'Geometric Composition': 0.11991835},\n",
       "  {'Minimal': 0.11453493},\n",
       "  {'Detailed': 0.068203911}],\n",
       " [{'Geometric Composition': 0.18202335},\n",
       "  {'Texture': 0.13059258},\n",
       "  {'Minimal': 0.12224399},\n",
       "  {'Bright': 0.091559596},\n",
       "  {'Melancholy': 0.058412801}],\n",
       " [{'Minimal': 0.17780131},\n",
       "  {'Geometric Composition': 0.14456463},\n",
       "  {'Bright': 0.14005892},\n",
       "  {'Horror': 0.10804387},\n",
       "  {'Detailed': 0.089948125}],\n",
       " [{'Bright': 0.16772291},\n",
       "  {'Detailed': 0.14749514},\n",
       "  {'Geometric Composition': 0.086241499},\n",
       "  {'Sunny': 0.072623231},\n",
       "  {'Texture': 0.059013907}],\n",
       " [{'Geometric Composition': 0.19296531},\n",
       "  {'Minimal': 0.14287888},\n",
       "  {'Bright': 0.13860342},\n",
       "  {'Texture': 0.10857093},\n",
       "  {'Detailed': 0.070430934}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_user('alexm.shots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplepython",
   "language": "python",
   "name": "simplepython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
